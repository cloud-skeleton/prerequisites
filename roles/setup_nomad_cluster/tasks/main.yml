---
- ansible.builtin.set_fact:
    node_ips: >-
      {{
        ansible_play_hosts
        | map('extract', hostvars, 'ansible_default_ipv4')
        | map(attribute='address')
        | reject('equalto', ansible_default_ipv4.address)
        | list
      }}
  name: Collect other nodes IPs

- community.general.ufw:
    from_ip: "{{ ip }}"
    log: true
    proto: "{{ proto }}"
    rule: allow
    to_port: "{{ port_no }}"
  loop: "{{ query('nested', node_ips, nomad_global_ports | map('split', '/') | list) }}"
  loop_control:
    loop_var: rule
  name: Allow Nomad global ports
  vars:
    ip: "{{ rule[0] }}"
    port_no: "{{ rule[1] }}"
    proto: "{{ rule[2] }}"

- ansible.builtin.set_fact:
    manager_ips: >-
      {{
        groups['manager']
        | map('extract', hostvars, 'ansible_default_ipv4')
        | map(attribute='address')
        | reject('equalto', ansible_default_ipv4.address)
        | list
      }}
  name: Collect other manager IPs

- community.general.ufw:
    from_ip: "{{ ip }}"
    log: true
    proto: "{{ proto }}"
    rule: allow
    to_port: "{{ port_no }}"
  loop: "{{ query('nested', manager_ips, nomad_server_ports | map('split', '/') | list) }}"
  loop_control:
    loop_var: rule
  name: Allow Nomad server ports
  vars:
    ip: "{{ rule[0] }}"
    port_no: "{{ rule[1] }}"
    proto: "{{ rule[2] }}"
  when: "'manager' in group_names"

- community.general.ufw:
    from_ip: 0.0.0.0/0
    log: true
    proto: "{{ proto }}"
    rule: allow
    to_port: "{{ port_no }}"
  loop: "{{ ingress_worker_ports | map('split', '/') }}"
  loop_control:
    loop_var: rule
  name: Allow ingress worker ports
  vars:
    port_no: "{{ rule[0] }}"
    proto: "{{ rule[1] }}"
  when: "'ingress_worker' in group_names"

- ansible.builtin.apt:
    autoclean: true
    autoremove: true
    clean: true
    install_recommends: true
    name: gnupg
    state: latest
    update_cache: true
  name: Ensure GPG is installed

- ansible.builtin.get_url:
    dest: /etc/apt/keyrings/hashicorp.asc
    force: true
    url: https://apt.releases.hashicorp.com/gpg
  name: Add Hashicorp GPG apt key to the store

- ansible.builtin.command:
    cmd: dpkg --print-architecture
  changed_when: false
  name: Gather Debian package architecture
  register: dpkg_arch

- ansible.builtin.apt_repository:
    repo: >-
      deb [arch={{ dpkg_arch.stdout }} signed-by=/etc/apt/keyrings/hashicorp.asc]
      https://apt.releases.hashicorp.com {{ ansible_facts.lsb.codename }} main
  name: Configure Hashicorp APT repository

- ansible.builtin.apt:
    autoclean: true
    autoremove: true
    clean: true
    install_recommends: true
    name: nomad
    state: latest
    update_cache: true
  name: Install Nomad package

- ansible.builtin.slurp:
    src: /home/ansible/.bashrc
  name: Read remote .bashrc into a variable
  register: bashrc_file

- ansible.builtin.set_fact:
    nomad_completion_present: >
      {{ 
        ('complete -C /usr/bin/nomad nomad')
        in (bashrc_file.content | b64decode)
      }}
  name: Check for existing Nomad autocomplete line in .bashrc

- ansible.builtin.command:
    cmd: nomad -autocomplete-install
  become: false
  changed_when: not nomad_completion_present
  name: Add Nomad autocomplete configuration

- ansible.builtin.copy:
    content: |
      [Unit]
      Description=Nomad
      Documentation=https://nomadproject.io/docs/
      Wants=network-online.target
      After=network-online.target

      # When using Nomad with Consul you should start Consul first, so that running
      # allocations using Consul are restored correctly during startup.
      #Wants=consul.service
      #After=consul.service

      ## Configure unit start rate limiting. Units which are started more than
      ## *burst* times within an *interval* time span are not permitted to start any
      ## more. Use `StartLimitIntervalSec` or `StartLimitInterval` (depending on
      ## systemd version) to configure the checking interval and `StartLimitBurst`
      ## to configure how many starts per interval are allowed. The values in the
      ## commented lines are defaults.

      # StartLimitBurst = 5

      ## StartLimitIntervalSec is used for systemd versions >= 230
      # StartLimitIntervalSec = 10s

      ## StartLimitInterval is used for systemd versions < 230
      # StartLimitInterval = 10s

      [Service]

      # Nomad clients need to be run as "root" whereas Nomad servers should be run as
      # the "nomad" user. Please change this if needed.
      User=nomad
      Group=nomad

      Type=notify
      EnvironmentFile=-/etc/nomad.d/nomad.env
      ExecReload=/bin/kill -HUP $MAINPID
      ExecStart=/usr/bin/nomad agent -config /etc/nomad.d
      KillMode=process
      KillSignal=SIGINT
      LimitNOFILE=65536
      LimitNPROC=infinity
      Restart=on-failure
      RestartSec=2

      TasksMax=infinity

      # Nomad Server agents should never be force killed,
      # so here we disable OOM (out of memory) killing for this unit.
      # However, you may wish to change this for Client agents, since
      # the workloads that Nomad places may be more important
      # than the Nomad agent itself.
      OOMScoreAdjust=-1000

      [Install]
      WantedBy=multi-user.target
    dest: /lib/systemd/system/nomad.service
    group: root
    mode: '0644'
    owner: root
  name: Configure Nomad service as server
  when: "'manager' in group_names"

- ansible.builtin.copy:
    content: |
      [Unit]
      Description=Nomad
      Documentation=https://nomadproject.io/docs/
      Wants=network-online.target
      After=network-online.target

      # When using Nomad with Consul you should start Consul first, so that running
      # allocations using Consul are restored correctly during startup.
      #Wants=consul.service
      #After=consul.service

      ## Configure unit start rate limiting. Units which are started more than
      ## *burst* times within an *interval* time span are not permitted to start any
      ## more. Use `StartLimitIntervalSec` or `StartLimitInterval` (depending on
      ## systemd version) to configure the checking interval and `StartLimitBurst`
      ## to configure how many starts per interval are allowed. The values in the
      ## commented lines are defaults.

      # StartLimitBurst = 5

      ## StartLimitIntervalSec is used for systemd versions >= 230
      # StartLimitIntervalSec = 10s

      ## StartLimitInterval is used for systemd versions < 230
      # StartLimitInterval = 10s

      [Service]

      # Nomad clients need to be run as "root" whereas Nomad servers should be run as
      # the "nomad" user. Please change this if needed.
      User=root
      Group=root

      Type=notify
      EnvironmentFile=-/etc/nomad.d/nomad.env
      ExecReload=/bin/kill -HUP $MAINPID
      ExecStart=/usr/bin/nomad agent -config /etc/nomad.d
      KillMode=process
      KillSignal=SIGINT
      LimitNOFILE=65536
      LimitNPROC=infinity
      Restart=on-failure
      RestartSec=2

      TasksMax=infinity

      # Nomad Server agents should never be force killed,
      # so here we disable OOM (out of memory) killing for this unit.
      # However, you may wish to change this for Client agents, since
      # the workloads that Nomad places may be more important
      # than the Nomad agent itself.
      OOMScoreAdjust=-1000

      [Install]
      WantedBy=multi-user.target
    dest: /lib/systemd/system/nomad.service
    group: root
    mode: '0644'
    owner: root
  name: Configure Nomad service as client
  when: "'manager' not in group_names"

# - community.docker.docker_swarm:
#     validate_certs: true
#   name: Initialize Swarm if needed
#   register: swarm_init
#   when: inventory_hostname == groups['manager'][0]

# - community.docker.docker_swarm_info:
#     validate_certs: true
#   name: Gather Swarm join tokens
#   register: swarm_info
#   when: inventory_hostname == groups['manager'][0]

# - ansible.builtin.set_fact:
#     manager_join_token: "{{ swarm_info.swarm_facts.JoinTokens.Manager }}"
#     worker_join_token: "{{ swarm_info.swarm_facts.JoinTokens.Worker }}"
#   name: Distribute Swarm join tokens
#   when: inventory_hostname == groups['manager'][0]

# - community.docker.docker_swarm:
#     join_token: "{{ hostvars[groups['manager'][0]]['manager_join_token'] }}"
#     remote_addrs:
#       - "{{ hostvars[groups['manager'][0]].ansible_default_ipv4.address }}"
#     state: join
#     validate_certs: true
#   name: Join Docker Swarm as manager node
#   when:
#     - "'manager' in group_names"
#     - inventory_hostname != groups['manager'][0]

# - community.docker.docker_swarm:
#     join_token: "{{ hostvars[groups['manager'][0]]['worker_join_token'] }}"
#     remote_addrs:
#       - "{{ hostvars[groups['manager'][0]].ansible_default_ipv4.address }}"
#     state: join
#     validate_certs: true
#   name: Join Docker Swarm as ingress worker node
#   when:
#     - "'ingress_worker' in group_names"

# - community.docker.docker_swarm:
#     join_token: "{{ hostvars[groups['manager'][0]]['worker_join_token'] }}"
#     remote_addrs:
#       - "{{ hostvars[groups['manager'][0]].ansible_default_ipv4.address }}"
#     state: join
#     validate_certs: true
#   name: Join Docker Swarm as main worker node
#   when:
#     - "'main_worker' in group_names"

# - community.docker.docker_node:
#     hostname: "{{ node }}"
#     labels:
#       eu.cloudskeleton.node: "true"
#       eu.cloudskeleton.node.type: "{{ hostvars[node].group_names[0] | replace('_','-') }}"
#     labels_state: replace
#     validate_certs: true
#   loop: "{{ groups['all'] }}"
#   loop_control:
#     loop_var: node
#   name: Label each node
#   when: inventory_hostname == groups['manager'][0]
...
